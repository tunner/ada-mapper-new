#!/usr/bin/env python3
"""
Minimal mapper generator (MVP sketch).

Reads mappings.json and emits a simple Ada package with
overloaded Map functions that map fields using Convert(...).

Assumptions (kept simple on purpose):
- Source types live in package `Types_From` (src/types_from.ads).
- Destination types live in package `Types_To` (src/types_to.ads).
- JSON lists mappings with `from`, `to`, `fields`.
- We generate files: src/position_mappers.ads and src/position_mappers.adb

Usage:
  python3 tools/gen_mapper.py [mappings.json] [outdir]
Defaults:
  mappings.json
  outdir = src
"""
from __future__ import annotations

import argparse
import json
import subprocess
import sys
from pathlib import Path
from typing import List, Optional
from records import parse_record_components
from arrays import parse_array_component_type, array_map_spec, array_map_body
from enums import enum_map_spec, enum_map_body
from generator import MapperGenerator
from scaffold import MappingRequest, MappingScaffolder
from types_provider import RegexTypesProvider


SPEC_TEMPLATE_HEADER = """with Types_From;
with Types_To;

package Position_Mappers is
   --  Generated by tools/gen_mapper.py — DO NOT EDIT.
"""

SPEC_TEMPLATE_FOOTER = """
end Position_Mappers;
"""


BODY_TEMPLATE_HEADER = """with Types_From; use Types_From;
with Types_To;   use Types_To;

package body Position_Mappers is
   --  Generated by tools/gen_mapper.py — DO NOT EDIT.
"""

BODY_TEMPLATE_FOOTER = """
end Position_Mappers;
"""


def gen_function_spec(src_type: str, dst_type: str) -> str:
    return f"   function Map (X : Types_From.{src_type}) return Types_To.{dst_type};\n"


def _gen_with_class(
    mg: MapperGenerator,
    src_type: str,
    dst_type: str,
    fields: dict[str, str],
    field_enum_overrides: dict[str, dict[str, str]] | None = None,
) -> str:
    return mg.gen_record_function_body(src_type, dst_type, fields, field_enum_overrides)


def run_compile(outdir: Path) -> int:
    """Compile the generated mapper to validate syntax using gnatmake only.

    Returns the subprocess return code. If gnatmake is not found, returns 127.
    """
    mapper = outdir / "position_mappers.adb"
    cmd = ["gnatmake", "-q", "-c", f"-I{outdir}", str(mapper)]
    try:
        res = subprocess.run(cmd, capture_output=True, text=True)
    except FileNotFoundError:
        sys.stderr.write(
            "Validation failed: gnatmake not found on PATH. Please install GNAT and ensure 'gnatmake' is available.\n"
        )
        return 127
    if res.returncode == 0:
        print("Validation: mapper compiles.")
    else:
        sys.stderr.write(res.stdout)
        sys.stderr.write(res.stderr)
    return res.returncode


def is_placeholder(value: object) -> bool:
    return isinstance(value, str) and value.startswith("<") and value.endswith(">")


def validate_mappings(mappings: List[dict], provider) -> List[str]:
    errors: List[str] = []
    mg = MapperGenerator(provider, set())

    mapping_pairs = set()
    for entry in mappings:
        src = entry.get("from")
        dst = entry.get("to")
        if isinstance(src, str) and isinstance(dst, str) and not is_placeholder(src):
            mapping_pairs.add((src.strip(), dst.strip()))
    mg.mapping_pairs = mapping_pairs

    for entry in mappings:
        errors.extend(_validate_mapping_entry(entry, mg, provider))
    return errors


def _validate_mapping_entry(entry: dict, mg: MapperGenerator, provider) -> List[str]:
    errors: List[str] = []

    name = entry.get("name")
    to_type_raw = entry.get("to")
    from_type_raw = entry.get("from")
    ctx = f"mapping '{name}'" if name else "a mapping entry"

    if not isinstance(to_type_raw, str) or not to_type_raw.strip():
        errors.append(f"{ctx}: destination type ('to') is missing or empty")
        return errors
    to_type = to_type_raw.strip()

    dest_fields = provider.get_record_fields("to", to_type)
    if not dest_fields:
        errors.append(f"{ctx}: destination type '{to_type}' not found or is not a record in destination specs")
        return errors

    if not isinstance(from_type_raw, str) or not from_type_raw.strip():
        errors.append(f"{ctx}: source type ('from') is missing")
        from_type = None
        from_fields = {}
    else:
        if is_placeholder(from_type_raw):
            errors.append(f"{ctx}: source type is still a placeholder '{from_type_raw}'")
        from_type = from_type_raw.strip()
        from_fields = provider.get_record_fields("from", from_type) or {}
        if not from_fields:
            errors.append(
                f"{ctx}: source type '{from_type}' not found or is not a record in source specs"
            )

    fields_entry = entry.get("fields")
    if not isinstance(fields_entry, dict):
        errors.append(f"{ctx}: 'fields' must be an object mapping destination fields to sources")
        return errors

    dest_lookup = {field.lower(): (field, dest_fields[field]) for field in dest_fields}
    dest_field_keys_lower = {field.lower() for field in fields_entry}
    missing_fields = [field for field in dest_fields if field.lower() not in dest_field_keys_lower]
    if missing_fields:
        errors.append(
            f"{ctx}: missing mappings for destination fields {', '.join(missing_fields)} in type '{to_type}'"
        )

    extra_fields = [field for field in fields_entry if field.lower() not in dest_lookup]
    for extra in extra_fields:
        errors.append(
            f"{ctx}: destination field '{extra}' does not exist in type '{to_type}'"
        )

    from_lookup = {name.lower(): (name, from_fields[name]) for name in from_fields}

    for dest_field, spec in fields_entry.items():
        lookup = dest_lookup.get(dest_field.lower())
        if not lookup:
            continue  # already reported as extra field
        dest_actual, dest_type_raw = lookup
        dest_type = dest_type_raw.strip()

        source_type: Optional[str] = None

        if isinstance(spec, str):
            spec_value = spec.strip()
            if is_placeholder(spec_value):
                errors.append(
                    f"{ctx}: field '{dest_actual}' still uses placeholder value '{spec_value}'"
                )
                continue
            if '.' in spec_value:
                if not from_type:
                    errors.append(
                        f"{ctx}: field '{dest_actual}' uses dotted path '{spec_value}' but source type is missing"
                    )
                else:
                    resolved = mg.resolve_src_path_type(from_type, spec_value)
                    if resolved is None:
                        errors.append(
                            f"{ctx}: field '{dest_actual}' references unknown source path '{spec_value}'"
                        )
                    else:
                        source_type = resolved.strip()
            else:
                if not from_lookup:
                    continue
                match = from_lookup.get(spec_value.lower())
                if not match:
                    errors.append(
                        f"{ctx}: field '{dest_actual}' references unknown source field '{spec_value}' in type '{from_type}'"
                    )
                else:
                    source_type = match[1].strip()

        elif isinstance(spec, dict):
            src_ref = spec.get("from") or spec.get("source") or spec.get("path")
            if not src_ref:
                errors.append(
                    f"{ctx}: field '{dest_actual}' mapping object must include 'from'/'source'/'path'"
                )
                continue
            if is_placeholder(src_ref):
                errors.append(
                    f"{ctx}: field '{dest_actual}' still uses placeholder value '{src_ref}'"
                )
                continue
            src_ref_str = str(src_ref).strip()
            if '.' in src_ref_str:
                if not from_type:
                    errors.append(
                        f"{ctx}: field '{dest_actual}' uses dotted path '{src_ref_str}' but source type is missing"
                    )
                else:
                    resolved = mg.resolve_src_path_type(from_type, src_ref_str)
                    if resolved is None:
                        errors.append(
                            f"{ctx}: field '{dest_actual}' references unknown source path '{src_ref_str}'"
                        )
                    else:
                        source_type = resolved.strip()
            else:
                if not from_lookup:
                    continue
                match = from_lookup.get(src_ref_str.lower())
                if not match:
                    errors.append(
                        f"{ctx}: field '{dest_actual}' references unknown source field '{src_ref_str}' in type '{from_type}'"
                    )
                else:
                    source_type = match[1].strip()

            enum_map = spec.get("enum_map")
            if enum_map is not None:
                if not isinstance(enum_map, dict):
                    errors.append(
                        f"{ctx}: field '{dest_actual}' has 'enum_map' but it must be an object"
                    )
                else:
                    dest_lits = provider.get_enum_literals("to", dest_type)
                    src_lits = provider.get_enum_literals("from", source_type) if source_type else None
                    if not dest_lits or not src_lits:
                        errors.append(
                            f"{ctx}: field '{dest_actual}' provides 'enum_map' but either source '{source_type}' or destination '{dest_type}' is not an enum"
                        )
                    else:
                        dest_lookup = {lit.lower(): lit for lit in dest_lits}
                        src_lookup = {lit.lower(): lit for lit in src_lits}
                        for raw_src, raw_dst in enum_map.items():
                            if not isinstance(raw_src, str) or not isinstance(raw_dst, str):
                                errors.append(
                                    f"{ctx}: field '{dest_actual}' has non-string enum_map entry {raw_src!r}: {raw_dst!r}"
                                )
                                continue
                            if raw_src.lower() not in src_lookup:
                                errors.append(
                                    f"{ctx}: field '{dest_actual}' enum_map references unknown source literal '{raw_src}'"
                                )
                            if raw_dst.lower() not in dest_lookup:
                                errors.append(
                                    f"{ctx}: field '{dest_actual}' enum_map targets unknown destination literal '{raw_dst}'"
                                )
        else:
            errors.append(
                f"{ctx}: field '{dest_actual}' has unsupported mapping value {spec!r}"
            )
            continue

        if source_type:
            dest_record = provider.get_record_fields("to", dest_type)
            if dest_record and not provider.get_record_fields("from", source_type):
                errors.append(
                    f"{ctx}: field '{dest_actual}' expects record type '{dest_type}' but source expression resolves to '{source_type}', which is not a record"
                )
            dest_array = provider.get_array_element_type("to", dest_type)
            if dest_array and not provider.get_array_element_type("from", source_type):
                errors.append(
                    f"{ctx}: field '{dest_actual}' expects array type '{dest_type}' but source expression resolves to '{source_type}', which is not an array"
                )

    return errors


def main():
    parser = argparse.ArgumentParser(description="Generate Ada mappers from JSON.")
    parser.add_argument("mappings", nargs="?", default="mappings.json", help="Path to mappings.json")
    parser.add_argument("outdir", nargs="?", default="src", help="Output directory (default: src)")
    parser.add_argument("--validate", action="store_true", help="Compile generated mapper to validate syntax")
    parser.add_argument("--provider", choices=["regex", "lal"], default="regex", help="Types provider backend (default: regex)")
    parser.add_argument(
        "--from-spec",
        default="types_from.ads",
        help="Ada spec file defining source types (default: types_from.ads relative to outdir)",
    )
    parser.add_argument(
        "--to-spec",
        default="types_to.ads",
        help="Ada spec file defining destination types (default: types_to.ads relative to outdir)",
    )
    parser.add_argument(
        "--init-json-map",
        nargs="+",
        metavar="NAME:FROM:TO",
        help="Initialize mappings.json scaffold for the given mapping specs (NAME:FROM:TO or FROM:TO)",
    )
    parser.add_argument(
        "--update-json-map",
        action="store_true",
        help="Update mappings.json by filling placeholders when matches are detected",
    )
    args = parser.parse_args()

    mappings_path = Path(args.mappings)
    outdir = Path(args.outdir)

    def resolve_spec(arg: str) -> Path:
        spec_path = Path(arg)
        if spec_path.is_absolute():
            return spec_path
        if spec_path.parent == Path('.'):
            return (outdir / spec_path)
        return spec_path

    types_from_ads = resolve_spec(args.from_spec)
    types_to_ads = resolve_spec(args.to_spec)
    if args.provider == "lal":
        from types_provider import LibadalangTypesProvider
        provider = LibadalangTypesProvider(types_from_ads, types_to_ads)
    else:
        provider = RegexTypesProvider(types_from_ads, types_to_ads)

    if args.init_json_map and args.update_json_map:
        print("--init-json-map and --update-json-map cannot be used together")
        sys.exit(1)

    if args.init_json_map:
        scaffolder = MappingScaffolder(provider)
        requests: list[MappingRequest] = []
        try:
            for spec in args.init_json_map:
                parts = spec.split(":")
                if len(parts) == 3:
                    name, from_type, to_type = parts
                elif len(parts) == 2:
                    from_type, to_type = parts
                    name = MappingScaffolder._default_name(to_type)
                else:
                    raise ValueError
                from_type_value = from_type.strip() or None
                requests.append(
                    MappingRequest(name=name.strip(), from_type=from_type_value, to_type=to_type.strip())
                )
        except ValueError:
            print(
                "Invalid mapping specification. Use NAME:FROM:TO or FROM:TO (e.g. Position:T_From:T_To)."
            )
            sys.exit(1)
        scaffold = scaffolder.build_map(requests)
        mappings_path.write_text(json.dumps(scaffold, indent=2) + "\n")
        print(f"Initialized {mappings_path}")
        return

    if args.update_json_map:
        scaffolder = MappingScaffolder(provider)
        data = json.loads(mappings_path.read_text())
        changed = scaffolder.update_map(data)
        if changed:
            mappings_path.write_text(json.dumps(data, indent=2) + "\n")
            print(f"Updated {mappings_path}")
        else:
            print("No changes made to", mappings_path)
        return

    spec_path = outdir / "position_mappers.ads"
    body_path = outdir / "position_mappers.adb"

    data = json.loads(mappings_path.read_text())
    mappings = data.get("mappings", [])
    if not mappings:
        print("No mappings found in", mappings_path)
        sys.exit(1)

    validation_errors = validate_mappings(mappings, provider)
    if validation_errors:
        sys.stderr.write("Unable to generate mapper due to invalid mappings:\n")
        for err in validation_errors:
            sys.stderr.write(f" - {err}\n")
        sys.exit(1)

    # Build spec and body content
    spec_parts = [SPEC_TEMPLATE_HEADER]
    body_parts = [BODY_TEMPLATE_HEADER]
    mg = MapperGenerator(provider, set())

    # Build a set of known mapping type pairs for nested delegation
    mg.mapping_pairs = set(
        (m.get("from"), m.get("to")) for m in mappings if m.get("from") and m.get("to")
    )

    for m in mappings:
        src_type = m["from"]
        dst_type = m["to"]
        fields_raw = m["fields"]
        # Normalize fields and collect per-field enum overrides
        fields = {}
        field_enum_overrides = {}
        for d, spec in fields_raw.items():
            if isinstance(spec, str):
                fields[d] = spec
            elif isinstance(spec, dict):
                src = spec.get("from") or spec.get("source") or spec.get("path")
                if not src:
                    raise ValueError(f"Field '{d}' mapping must include 'from'")
                fields[d] = src
                enum_map = spec.get("enum_map")
                if isinstance(enum_map, dict):
                    field_enum_overrides[d] = enum_map
            else:
                raise ValueError(f"Unsupported field mapping for '{d}': {spec!r}")

        spec_parts.append(gen_function_spec(src_type, dst_type))
        body_parts.append(_gen_with_class(mg, src_type, dst_type, fields, field_enum_overrides))

    # Expand array mapping pairs transitively for nested arrays
    mg.expand_array_pairs_transitively()

    # Add Map specs for detected array pairs
    for src_arr, dst_arr in sorted(mg.needed_array_maps):
        spec_parts.append(array_map_spec(src_arr, dst_arr))

    # Add Map specs for detected enum pairs
    for src_enum, dst_enum in sorted(mg.needed_enum_maps):
        spec_parts.append(enum_map_spec(src_enum, dst_enum))

    # Emit array Map bodies (element-wise mapping with recursion)
    for src_arr, dst_arr in sorted(mg.needed_array_maps):
        body_parts.append(array_map_body(mg, src_arr, dst_arr))

    # Emit enum Map bodies
    for src_enum, dst_enum in sorted(mg.needed_enum_maps):
        body_parts.append(enum_map_body(mg, src_enum, dst_enum))
    spec_parts.append(SPEC_TEMPLATE_FOOTER)
    body_parts.append(BODY_TEMPLATE_FOOTER)

    outdir.mkdir(parents=True, exist_ok=True)
    spec_path.write_text("".join(spec_parts))
    body_path.write_text("".join(body_parts))

    print(f"Wrote {spec_path}")
    print(f"Wrote {body_path}")

    if args.validate:
        code = run_compile(outdir)
        if code != 0:
            sys.exit(code)


if __name__ == "__main__":
    main()
